// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Databricks
{
    /// <summary>
    /// Use `databricks.Pipeline` to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
    /// 
    /// ## Example Usage
    /// 
    /// ```csharp
    /// using Pulumi;
    /// using Databricks = Pulumi.Databricks;
    /// 
    /// class MyStack : Stack
    /// {
    ///     public MyStack()
    ///     {
    ///         var dltDemo = new Databricks.Notebook("dltDemo", new Databricks.NotebookArgs
    ///         {
    ///         });
    ///         //...
    ///         var @this = new Databricks.Pipeline("this", new Databricks.PipelineArgs
    ///         {
    ///             Storage = "/test/first-pipeline",
    ///             Configuration = 
    ///             {
    ///                 { "key1", "value1" },
    ///                 { "key2", "value2" },
    ///             },
    ///             Clusters = 
    ///             {
    ///                 new Databricks.Inputs.PipelineClusterArgs
    ///                 {
    ///                     Label = "default",
    ///                     NumWorkers = 2,
    ///                     CustomTags = 
    ///                     {
    ///                         { "cluster_type", "default" },
    ///                     },
    ///                 },
    ///                 new Databricks.Inputs.PipelineClusterArgs
    ///                 {
    ///                     Label = "maintenance",
    ///                     NumWorkers = 1,
    ///                     CustomTags = 
    ///                     {
    ///                         { "cluster_type", "maintenance" },
    ///                     },
    ///                 },
    ///             },
    ///             Libraries = 
    ///             {
    ///                 new Databricks.Inputs.PipelineLibraryArgs
    ///                 {
    ///                     Notebook = new Databricks.Inputs.PipelineLibraryNotebookArgs
    ///                     {
    ///                         Path = dltDemo.Id,
    ///                     },
    ///                 },
    ///             },
    ///             Filters = new Databricks.Inputs.PipelineFiltersArgs
    ///             {
    ///                 Includes = 
    ///                 {
    ///                     "com.databricks.include",
    ///                 },
    ///                 Excludes = 
    ///                 {
    ///                     "com.databricks.exclude",
    ///                 },
    ///             },
    ///             Continuous = false,
    ///         });
    ///     }
    /// 
    /// }
    /// ```
    /// ## Related Resources
    /// 
    /// The following resources are often used in the same context:
    /// 
    /// * End to end workspace management guide.
    /// * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
    /// * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
    /// * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
    /// 
    /// ## Import
    /// 
    /// The resource job can be imported using the id of the pipeline bash
    /// 
    /// ```sh
    ///  $ pulumi import databricks:index/pipeline:Pipeline this &lt;pipeline-id&gt;
    /// ```
    /// </summary>
    [DatabricksResourceType("databricks:index/pipeline:Pipeline")]
    public partial class Pipeline : Pulumi.CustomResource
    {
        [Output("allowDuplicateNames")]
        public Output<bool?> AllowDuplicateNames { get; private set; } = null!;

        /// <summary>
        /// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline.
        /// </summary>
        [Output("clusters")]
        public Output<ImmutableArray<Outputs.PipelineCluster>> Clusters { get; private set; } = null!;

        /// <summary>
        /// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
        /// </summary>
        [Output("configuration")]
        public Output<ImmutableDictionary<string, object>?> Configuration { get; private set; } = null!;

        /// <summary>
        /// A flag indicating whether to run the pipeline continuously. The default value is `false`.
        /// </summary>
        [Output("continuous")]
        public Output<bool?> Continuous { get; private set; } = null!;

        [Output("filters")]
        public Output<Outputs.PipelineFilters> Filters { get; private set; } = null!;

        [Output("id")]
        public Output<string> Id { get; private set; } = null!;

        /// <summary>
        /// blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have `path` attribute.
        /// </summary>
        [Output("libraries")]
        public Output<ImmutableArray<Outputs.PipelineLibrary>> Libraries { get; private set; } = null!;

        /// <summary>
        /// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
        /// </summary>
        [Output("name")]
        public Output<string> Name { get; private set; } = null!;

        /// <summary>
        /// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location.
        /// </summary>
        [Output("storage")]
        public Output<string?> Storage { get; private set; } = null!;

        /// <summary>
        /// The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
        /// </summary>
        [Output("target")]
        public Output<string?> Target { get; private set; } = null!;

        [Output("url")]
        public Output<string> Url { get; private set; } = null!;


        /// <summary>
        /// Create a Pipeline resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public Pipeline(string name, PipelineArgs args, CustomResourceOptions? options = null)
            : base("databricks:index/pipeline:Pipeline", name, args ?? new PipelineArgs(), MakeResourceOptions(options, ""))
        {
        }

        private Pipeline(string name, Input<string> id, PipelineState? state = null, CustomResourceOptions? options = null)
            : base("databricks:index/pipeline:Pipeline", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing Pipeline resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static Pipeline Get(string name, Input<string> id, PipelineState? state = null, CustomResourceOptions? options = null)
        {
            return new Pipeline(name, id, state, options);
        }
    }

    public sealed class PipelineArgs : Pulumi.ResourceArgs
    {
        [Input("allowDuplicateNames")]
        public Input<bool>? AllowDuplicateNames { get; set; }

        [Input("clusters")]
        private InputList<Inputs.PipelineClusterArgs>? _clusters;

        /// <summary>
        /// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline.
        /// </summary>
        public InputList<Inputs.PipelineClusterArgs> Clusters
        {
            get => _clusters ?? (_clusters = new InputList<Inputs.PipelineClusterArgs>());
            set => _clusters = value;
        }

        [Input("configuration")]
        private InputMap<object>? _configuration;

        /// <summary>
        /// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
        /// </summary>
        public InputMap<object> Configuration
        {
            get => _configuration ?? (_configuration = new InputMap<object>());
            set => _configuration = value;
        }

        /// <summary>
        /// A flag indicating whether to run the pipeline continuously. The default value is `false`.
        /// </summary>
        [Input("continuous")]
        public Input<bool>? Continuous { get; set; }

        [Input("filters", required: true)]
        public Input<Inputs.PipelineFiltersArgs> Filters { get; set; } = null!;

        [Input("id")]
        public Input<string>? Id { get; set; }

        [Input("libraries")]
        private InputList<Inputs.PipelineLibraryArgs>? _libraries;

        /// <summary>
        /// blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have `path` attribute.
        /// </summary>
        public InputList<Inputs.PipelineLibraryArgs> Libraries
        {
            get => _libraries ?? (_libraries = new InputList<Inputs.PipelineLibraryArgs>());
            set => _libraries = value;
        }

        /// <summary>
        /// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location.
        /// </summary>
        [Input("storage")]
        public Input<string>? Storage { get; set; }

        /// <summary>
        /// The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
        /// </summary>
        [Input("target")]
        public Input<string>? Target { get; set; }

        public PipelineArgs()
        {
        }
    }

    public sealed class PipelineState : Pulumi.ResourceArgs
    {
        [Input("allowDuplicateNames")]
        public Input<bool>? AllowDuplicateNames { get; set; }

        [Input("clusters")]
        private InputList<Inputs.PipelineClusterGetArgs>? _clusters;

        /// <summary>
        /// blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline.
        /// </summary>
        public InputList<Inputs.PipelineClusterGetArgs> Clusters
        {
            get => _clusters ?? (_clusters = new InputList<Inputs.PipelineClusterGetArgs>());
            set => _clusters = value;
        }

        [Input("configuration")]
        private InputMap<object>? _configuration;

        /// <summary>
        /// An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
        /// </summary>
        public InputMap<object> Configuration
        {
            get => _configuration ?? (_configuration = new InputMap<object>());
            set => _configuration = value;
        }

        /// <summary>
        /// A flag indicating whether to run the pipeline continuously. The default value is `false`.
        /// </summary>
        [Input("continuous")]
        public Input<bool>? Continuous { get; set; }

        [Input("filters")]
        public Input<Inputs.PipelineFiltersGetArgs>? Filters { get; set; }

        [Input("id")]
        public Input<string>? Id { get; set; }

        [Input("libraries")]
        private InputList<Inputs.PipelineLibraryGetArgs>? _libraries;

        /// <summary>
        /// blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have `path` attribute.
        /// </summary>
        public InputList<Inputs.PipelineLibraryGetArgs> Libraries
        {
            get => _libraries ?? (_libraries = new InputList<Inputs.PipelineLibraryGetArgs>());
            set => _libraries = value;
        }

        /// <summary>
        /// A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
        /// </summary>
        [Input("name")]
        public Input<string>? Name { get; set; }

        /// <summary>
        /// A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location.
        /// </summary>
        [Input("storage")]
        public Input<string>? Storage { get; set; }

        /// <summary>
        /// The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
        /// </summary>
        [Input("target")]
        public Input<string>? Target { get; set; }

        [Input("url")]
        public Input<string>? Url { get; set; }

        public PipelineState()
        {
        }
    }
}
